import os
import sys
from ctransformers import AutoModelForCausalLM


def load_llm():
    """load a pre-trained LLM"""
    model_repo = "TheBloke/Llama-2-7b-Chat-GGUF"  # name of a Hugging Face Hub model repo
    model_file = "llama-2-7b-chat.Q4_K_M.gguf"   # name of the model file in repo
    print(f'loading model "{model_repo}/{model_file}" ...')
    # to work offline, you may set `local_files_only` to True once the model files have been downloaded.
    llm = AutoModelForCausalLM.from_pretrained(model_repo, model_file=model_file, local_files_only=False)
    return llm


def query_llm(llm, prompt, max_new_tokens=256, verbose=True, seed=101, **kwargs):
    """use a pre-trained LLM to generate (and stream) text w.r.t. the given prompt.

    Args:
        llm (LLM): a pre-trained LLM
        prompt (str): the prompt for text generation
        max_new_tokens (int): the maximum number of new tokens may be generated by the LLM
        verbose (boolean): print additional details if True
        seed (int): the seed value to use for sampling tokens

    Returns:
        str: the generated (and streamed) text
    """
    assert len(prompt.strip()) > 0
    kwargs.update({
        'max_new_tokens': max_new_tokens,
        'seed': seed,
        'stream': True,  # whether to stream the generated text
        'reset': True,  # whether to reset the model state before generating text
    })
    
    if verbose:
        sys.stdout.write(f'\n{prompt}')

    ans = []
    for text in llm(prompt, **kwargs):
        print(text, end="", flush=True)
        ans.append(text)
    sys.stdout.write('\n')
    return "".join(ans)


def get_prompt(query, context=None, num_word=50):
    """create a prompt for the given query.

    Args:
        query (str): the query string
        context (str): optional, the context string
        num_word (int): the (approximate) number of words in the generated text

    Returns:
        str: the prompt string
    """
    if context is None:
        return f'Answer the question below in text using about {num_word} words, your answer should be in bullet points.\n\nQuestion:\n{query}\n\nAnswer:\n'
    
    # TODO: create a prompt for the given query and context
    prompt = ""


    return prompt


if __name__ == '__main__':
    llm = load_llm()
    for query in [
        'Is nuclear power plant eco-friendly?',
        'How to stay safe during severe weather?',
    ]: 
        # directly query the pre-trained LLM
        query_llm(llm, get_prompt(query), verbose=True, seed=101)

